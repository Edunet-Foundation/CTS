{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Stat and Transformation Dataframes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU3Ul-aoOdZu"
      },
      "source": [
        "## optional Step \n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if ('sc' in locals() or 'sc' in globals()):\n",
        "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUlDNsiOdZv",
        "outputId": "5c21f918-647f-4a22-f033-7419ee26e06c"
      },
      "source": [
        "#optional step\n",
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 11.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=d0127cad44a050e7a4c47ede675e77d7686ff5d5274d39ca592099a2e1b5179f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rAFAAdTOdZv"
      },
      "source": [
        "#optional step\n",
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nl1tBMAOdZv"
      },
      "source": [
        "#optional step\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "IsA89eLpOdZv"
      },
      "source": [
        " In this exercise you’ll read a DataFrame in order to perform a simple statistical analysis.  let’s get started.\n",
        "\n",
        "Let’s create a data frame from a remote file by downloading it:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqP7O0zPOdZ1",
        "outputId": "12fb5599-e7d9-44cf-e1e7-6ab30f2772da"
      },
      "source": [
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-09 05:46:56--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/IBM/claimed/raw/master/hmp.parquet [following]\n",
            "--2021-11-09 05:46:56--  https://github.com/IBM/claimed/raw/master/hmp.parquet\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IBM/claimed/master/hmp.parquet [following]\n",
            "--2021-11-09 05:46:56--  https://raw.githubusercontent.com/IBM/claimed/master/hmp.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 932997 (911K) [application/octet-stream]\n",
            "Saving to: ‘hmp.parquet’\n",
            "\n",
            "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2021-11-09 05:46:56 (120 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCk_5DfEPEKb"
      },
      "source": [
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKhsAAeTOdZ3"
      },
      "source": [
        "Let’s have a look at the data set first. This dataset contains sensor recordings from different movement activities as we will see in the next week’s lectures. X, Y and Z contain accelerometer sensor values whereas the class field contains information about which movement has been recorded. The source field is optional and can be used for data lineage since it contains the file name of the original file where the particular row was imported from.\n",
        "\n",
        "More details on the data set can be found here:\n",
        "https://github.com/wchill/HMP_Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4q74jpkOdZ3",
        "outputId": "d74d38e4-288d-4f0e-944e-31e3e95027f7"
      },
      "source": [
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------------------+-----------+\n",
            "|  x|  y|  z|              source|      class|\n",
            "+---+---+---+--------------------+-----------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "+---+---+---+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- x: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            " |-- z: integer (nullable = true)\n",
            " |-- source: string (nullable = true)\n",
            " |-- class: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwGCY9qqaiz6",
        "outputId": "c26b119b-5f5e-4d14-8d54-c0fb3ee2ae2d"
      },
      "source": [
        "df.select(\"x\").show(5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|  x|\n",
            "+---+\n",
            "| 22|\n",
            "| 22|\n",
            "| 22|\n",
            "| 22|\n",
            "| 21|\n",
            "+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttlxEu2Ha2d7",
        "outputId": "587bbfe0-ad35-4ab5-e3b0-e942a7886222"
      },
      "source": [
        "df.select(\"x\",\"y\",'z').show(5)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "|  x|  y|  z|\n",
            "+---+---+---+\n",
            "| 22| 49| 35|\n",
            "| 22| 49| 35|\n",
            "| 22| 52| 35|\n",
            "| 22| 52| 35|\n",
            "| 21| 52| 34|\n",
            "+---+---+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQVHS6rIbOrg",
        "outputId": "58eca5a5-d1d9-413a-aefd-cc68603cf886"
      },
      "source": [
        "df.filter(df['x'] > 21).show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------------------+-----------+\n",
            "|  x|  y|  z|              source|      class|\n",
            "+---+---+---+--------------------+-----------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 24| 51| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 50| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 51| 36|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 50| 38|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 51| 39|Accelerometer-201...|Brush_teeth|\n",
            "| 26| 49| 40|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 49| 41|Accelerometer-201...|Brush_teeth|\n",
            "| 25| 47| 42|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 45| 41|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 47| 41|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 47| 42|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 47| 42|Accelerometer-201...|Brush_teeth|\n",
            "+---+---+---+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkOVxffWOdZ4"
      },
      "source": [
        "This is a classical classification data set. One thing we always do during data analysis is checking if the classes are balanced. In other words, if there are more or less the same number of example in each class. Let’s find out by a simple aggregation using SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3xGWdMlOdZ4",
        "outputId": "fe752ca1-c0fa-453e-dc56-928418e51ee3"
      },
      "source": [
        "spark.sql('select class,count(*) from df group by class').show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------+\n",
            "|         class|count(1)|\n",
            "+--------------+--------+\n",
            "| Use_telephone|   15225|\n",
            "| Standup_chair|   25417|\n",
            "|      Eat_meat|   31236|\n",
            "|     Getup_bed|   45801|\n",
            "|   Drink_glass|   42792|\n",
            "|    Pour_water|   41673|\n",
            "|     Comb_hair|   23504|\n",
            "|          Walk|   92254|\n",
            "|  Climb_stairs|   40258|\n",
            "| Sitdown_chair|   25036|\n",
            "|   Liedown_bed|   11446|\n",
            "|Descend_stairs|   15375|\n",
            "|   Brush_teeth|   29829|\n",
            "|      Eat_soup|    6683|\n",
            "+--------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWboQV-YQLGi",
        "outputId": "76a253b8-a4e0-4694-b5cd-a358dd5eb760"
      },
      "source": [
        "spark.sql('select x from df  ').show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|  x|\n",
            "+---+\n",
            "| 22|\n",
            "| 22|\n",
            "| 22|\n",
            "| 22|\n",
            "| 21|\n",
            "| 22|\n",
            "| 20|\n",
            "| 22|\n",
            "| 22|\n",
            "| 22|\n",
            "| 21|\n",
            "| 20|\n",
            "| 21|\n",
            "| 21|\n",
            "| 20|\n",
            "| 18|\n",
            "| 19|\n",
            "| 16|\n",
            "| 18|\n",
            "| 18|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6FS8rSCQwVX",
        "outputId": "60a29b20-2b46-4fed-c85c-23074d9437b0"
      },
      "source": [
        "spark.sql('select count(*) from df ').show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  446529|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxnmy9-kOdZ6",
        "outputId": "b7b24d49-51b5-4ecb-d85e-5049d59b6692"
      },
      "source": [
        "df.groupBy('class').count().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|         class|count|\n",
            "+--------------+-----+\n",
            "| Use_telephone|15225|\n",
            "| Standup_chair|25417|\n",
            "|      Eat_meat|31236|\n",
            "|     Getup_bed|45801|\n",
            "|   Drink_glass|42792|\n",
            "|    Pour_water|41673|\n",
            "|     Comb_hair|23504|\n",
            "|          Walk|92254|\n",
            "|  Climb_stairs|40258|\n",
            "| Sitdown_chair|25036|\n",
            "|   Liedown_bed|11446|\n",
            "|Descend_stairs|15375|\n",
            "|   Brush_teeth|29829|\n",
            "|      Eat_soup| 6683|\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNRLnziOdZ7"
      },
      "source": [
        " we can aggregate further to obtain some quantitative metrics on the imbalance like, min, max, mean and standard deviation. If we divide max by min we get a measure called minmax ration which tells us something about the relationship between the smallest and largest class. Again, let’s first use SQL for those of you familiar with SQL. Don’t be scared, we’re used nested sub-selects, basically selecting from a result of a SQL query like it was a table. All within on SQL statement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVMUOdSYOdZ7",
        "outputId": "333d66c3-6494-47d5-d329-f9012a7fff0e"
      },
      "source": [
        "spark.sql('''\n",
        "    select \n",
        "        *,\n",
        "        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n",
        "        from (\n",
        "            select \n",
        "                min(ct) as min, -- compute minimum value of all classes\n",
        "                max(ct) as max, -- compute maximum value of all classes\n",
        "                mean(ct) as mean, -- compute mean between all classes\n",
        "                stddev(ct) as stddev -- compute standard deviation between all classes\n",
        "                from (\n",
        "                    select\n",
        "                        count(*) as ct -- count the number of rows per class and rename it to ct\n",
        "                        from df -- access the temporary query table called df backed by DataFrame df\n",
        "                        group by class -- aggrecate over class\n",
        "                )\n",
        "        )   \n",
        "''').show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+------------------+------------------+-----------------+\n",
            "| min|  max|              mean|            stddev|      minmaxratio|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIBYGMZ6OdZ8"
      },
      "source": [
        "The same query can be expressed using the DataFrame API. Again, don’t be scared. It’s just a sequential expression of transformation steps. You now an choose which syntax you like better.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbeFySR9OdZ8",
        "outputId": "7343f4a3-2f86-48e3-be4c-c83feddbb1f2"
      },
      "source": [
        "from pyspark.sql.functions import col, min, max, mean, stddev\n",
        "\n",
        "df \\\n",
        "    .groupBy('class') \\\n",
        "    .count() \\\n",
        "    .select([ \n",
        "        min(col(\"count\")).alias('min'), \n",
        "        max(col(\"count\")).alias('max'), \n",
        "        mean(col(\"count\")).alias('mean'), \n",
        "        stddev(col(\"count\")).alias('stddev') \n",
        "    ]) \\\n",
        "    .select([\n",
        "        col('*'),\n",
        "        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n",
        "    ]) \\\n",
        "    .show()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+------------------+------------------+-----------------+\n",
            "| min|  max|              mean|            stddev|      minmaxratio|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}